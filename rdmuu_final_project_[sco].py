# -*- coding: utf-8 -*-
"""RDMUU FINAL PROJECT [SCO].ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1n25szFsZdlvssjG0kulfzs4nKv89lSf0
"""

# Define the Supply Chain MDP (Markov Decision Process) class
class SupplyChainMDP:
    def __init__(self, num_suppliers, num_warehouses, num_retailers,
                 max_inventory, max_demand, lead_time_max=3):
        """
        Initialize the Supply Chain MDP

        Parameters:
        -----------
        num_suppliers: int
            Number of suppliers in the supply chain
        num_warehouses: int
            Number of warehouses in the supply chain
        num_retailers: int
            Number of retailers in the supply chain
        max_inventory: int
            Maximum inventory capacity at each location
        max_demand: int
            Maximum possible demand at retailers
        lead_time_max: int
            Maximum lead time for deliveries
        """
        self.num_suppliers = num_suppliers
        self.num_warehouses = num_warehouses
        self.num_retailers = num_retailers
        self.max_inventory = max_inventory
        self.max_demand = max_demand
        self.lead_time_max = lead_time_max

        # Define state space dimensions
        self.inventory_levels = np.zeros((num_suppliers + num_warehouses + num_retailers,))
        self.pending_orders = []  # List to track orders in transit

        # Define action space (order quantities)
        self.max_order = max_inventory  # Maximum order size

        # Initialize transition probabilities
        self.demand_probs = self._initialize_demand_probabilities()
        self.lead_time_probs = self._initialize_lead_time_probabilities()

        # Cost parameters
        self.holding_cost = 1.0  # Cost per unit per time period
        self.stockout_cost = 10.0  # Cost per unit of unfulfilled demand
        self.order_cost = 5.0  # Fixed cost per order
        self.transport_cost = 0.5  # Cost per unit per distance

        # Create network structure
        self.network = self._create_network()

    def _initialize_demand_probabilities(self):
        """Initialize demand probability distributions for retailers"""
        # For simplicity, using uniform distribution initially
        # In a real system, this would be based on historical data
        demand_probs = np.ones((self.num_retailers, self.max_demand + 1)) / (self.max_demand + 1)
        return demand_probs

    def _initialize_lead_time_probabilities(self):
        """Initialize lead time probability distributions"""
        # For simplicity, using uniform distribution initially
        lead_time_probs = np.ones((self.lead_time_max + 1,)) / (self.lead_time_max + 1)
        return lead_time_probs

    def _create_network(self):
        """Create a network representation of the supply chain"""
        G = nx.DiGraph()

        # Add supplier nodes
        for i in range(self.num_suppliers):
            G.add_node(f"S{i}", type="supplier", inventory=self.max_inventory)

        # Add warehouse nodes
        for i in range(self.num_warehouses):
            G.add_node(f"W{i}", type="warehouse", inventory=self.max_inventory//2)

        # Add retailer nodes
        for i in range(self.num_retailers):
            G.add_node(f"R{i}", type="retailer", inventory=self.max_inventory//4)

        # Connect suppliers to warehouses (fully connected for simplicity)
        for i in range(self.num_suppliers):
            for j in range(self.num_warehouses):
                # Random distance between 1 and 10
                distance = np.random.randint(1, 11)
                G.add_edge(f"S{i}", f"W{j}", distance=distance,
                           lead_time=min(1 + distance//3, self.lead_time_max))

        # Connect warehouses to retailers (fully connected for simplicity)
        for i in range(self.num_warehouses):
            for j in range(self.num_retailers):
                # Random distance between 1 and 5
                distance = np.random.randint(1, 6)
                G.add_edge(f"W{i}", f"R{j}", distance=distance,
                           lead_time=min(1 + distance//3, self.lead_time_max))

        return G

    def visualize_network(self):
        """Visualize the supply chain network"""
        plt.figure(figsize=(12, 8))

        # Create position layout
        pos = {}

        # Position suppliers on the left
        for i in range(self.num_suppliers):
            pos[f"S{i}"] = (0, i - self.num_suppliers/2 + 0.5)

        # Position warehouses in the middle
        for i in range(self.num_warehouses):
            pos[f"W{i}"] = (1, i - self.num_warehouses/2 + 0.5)

        # Position retailers on the right
        for i in range(self.num_retailers):
            pos[f"R{i}"] = (2, i - self.num_retailers/2 + 0.5)

        # Draw nodes with different colors for each type
        supplier_nodes = [n for n in self.network.nodes if n.startswith('S')]
        warehouse_nodes = [n for n in self.network.nodes if n.startswith('W')]
        retailer_nodes = [n for n in self.network.nodes if n.startswith('R')]

        # Draw edges with lead time as labels
        edge_labels = {(u, v): f"LT:{d['lead_time']}" for u, v, d in self.network.edges(data=True)}

        # Draw the network
        nx.draw_networkx_nodes(self.network, pos, nodelist=supplier_nodes,
                              node_color='#766CDB', node_size=700, label='Suppliers')
        nx.draw_networkx_nodes(self.network, pos, nodelist=warehouse_nodes,
                              node_color='#DA847C', node_size=700, label='Warehouses')
        nx.draw_networkx_nodes(self.network, pos, nodelist=retailer_nodes,
                              node_color='#7CD9A5', node_size=700, label='Retailers')

        nx.draw_networkx_edges(self.network, pos, width=1.5, edge_color='#333333',
                              arrowsize=15, arrowstyle='->')
        nx.draw_networkx_labels(self.network, pos, font_size=14, font_family='Lato')
        nx.draw_networkx_edge_labels(self.network, pos, edge_labels=edge_labels,
                                    font_size=10, font_family='Lato')

        plt.title('Supply Chain Network Structure', fontsize=20, fontweight='semibold',
                 fontname='Lato', pad=15, color='#222222')
        plt.legend(fontsize=12)
        plt.axis('off')
        plt.tight_layout()
        plt.show()

# Create a simple supply chain instance
np.random.seed(42)  # For reproducibility
supply_chain = SupplyChainMDP(
    num_suppliers=3,
    num_warehouses=2,
    num_retailers=4,
    max_inventory=100,
    max_demand=20
)

# Visualize the network
supply_chain.visualize_network()

print("Supply Chain MDP model initialized successfully!")

# Enhanced Supply Chain MDP with all expected outputs
import numpy as np
import matplotlib.pyplot as plt
import networkx as nx
from matplotlib.animation import FuncAnimation
import time

# Enhanced SupplyChainMDP class
class SupplyChainMDP:
    def __init__(self, states, actions, transition_probabilities, rewards, discount=0.9, theta=1e-5):
        self.states = states
        self.actions = actions
        self.P = transition_probabilities
        self.R = rewards
        self.discount = discount
        self.theta = theta
        # Initialize value function and policy
        self.V = {s: 0.0 for s in states}
        self.policy = {s: actions[0] for s in states}
        # For tracking policy improvements
        self.policy_history = []
        self.value_history = []

    def run_value_iteration(self, track_progress=True):
        max_iter = 1000
        iteration = 0

        # Store initial policy
        if track_progress:
            self.policy_history.append(self.policy.copy())
            self.value_history.append(self.V.copy())

        while True:
            delta = 0
            current_policy = self.policy.copy()

            for s in self.states:
                v = self.V[s]
                # Compute new value for state s
                max_val = -np.inf
                best_action = None
                for a in self.actions:
                    # Get reward for this state-action
                    r = self.R.get(s, {}).get(a, 0)
                    # Sum over next states
                    expected_value = 0
                    for s_prime, prob in self.P.get(s, {}).get(a, {}).items():
                        expected_value += prob * self.V[s_prime]
                    tmp = r + self.discount * expected_value
                    if tmp > max_val:
                        max_val = tmp
                        best_action = a
                self.V[s] = max_val
                self.policy[s] = best_action
                delta = max(delta, abs(v - max_val))

            # Store policy and value function if they changed
            if track_progress and (current_policy != self.policy or iteration % 10 == 0):
                self.policy_history.append(self.policy.copy())
                self.value_history.append(self.V.copy())

            iteration += 1
            if delta < self.theta or iteration >= max_iter:
                break

        print('Value iteration converged in ' + str(iteration) + ' iterations.')
        return self.V, self.policy

    def visualize_policy(self):
        # Create a simple bar chart of state values
        states = self.states
        values = [self.V[s] for s in states]
        policies = [self.policy[s] for s in states]

        fig, ax = plt.subplots(figsize=(9, 6))
        plt.subplots_adjust(left=0.15, right=0.85, top=0.85, bottom=0.15)
        bars = ax.bar(states, values, color='#766CDB')
        ax.set_title('State Value Function', pad=15, fontsize=20, fontweight='semibold', color='#222222')
        ax.set_xlabel('States', labelpad=10, fontsize=16, fontweight='medium', color='#333333')
        ax.set_ylabel('Value', labelpad=10, fontsize=16, fontweight='medium', color='#333333')
        ax.tick_params(axis='both', labelsize=14, colors='#555555')
        ax.set_axisbelow(True)

        # Annotate bars with the best action from the policy
        for bar, action in zip(bars, policies):
            height = bar.get_height()
            ax.text(bar.get_x() + bar.get_width()/2, height, str(action), ha='center', va='bottom', fontsize=12, color='#333333')

        # Add grid lines
        ax.grid(True, linestyle='--', color='#E0E0E0', zorder=0)

        plt.show()
        print('Policy visualization complete.')

    def visualize_optimal_routes(self):
        """Visualize the optimal delivery routes based on the policy"""
        G = nx.DiGraph()

        # Add all states as nodes
        for state in self.states:
            G.add_node(state)

        # Add edges based on policy and transition probabilities
        for state in self.states:
            action = self.policy[state]
            for next_state, prob in self.P[state][action].items():
                if prob > 0:
                    G.add_edge(state, next_state, weight=prob, action=action)

        # Create positions for nodes
        pos = nx.spring_layout(G, seed=42)

        # Create figure
        fig, ax = plt.subplots(figsize=(9, 6))
        plt.subplots_adjust(left=0.15, right=0.85, top=0.85, bottom=0.15)

        # Draw nodes
        nx.draw_networkx_nodes(G, pos, node_color='#766CDB', node_size=500, ax=ax)

        # Draw edges with varying thickness based on probability
        for u, v, data in G.edges(data=True):
            nx.draw_networkx_edges(G, pos, edgelist=[(u, v)], width=data['weight']*3,
                                  edge_color='#DA847C', arrows=True, arrowsize=15, ax=ax)

        # Draw labels
        nx.draw_networkx_labels(G, pos, font_size=14, font_color='white')

        # Draw edge labels (actions)
        edge_labels = {(u, v): data['action'] for u, v, data in G.edges(data=True)}
        nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=12)

        ax.set_title('Optimal Delivery Routes', pad=15, fontsize=20, fontweight='semibold', color='#222222')
        ax.axis('off')

        plt.show()
        print('Optimal routes visualization complete.')

    def visualize_policy_improvements(self):
        """Visualize how the policy improved over iterations"""
        if not self.policy_history:
            print("No policy history available. Run value iteration with track_progress=True first.")
            return

        # Track the action for each state across iterations
        state_progress = {s: [] for s in self.states}
        for policy in self.policy_history:
            for s in self.states:
                state_progress[s].append(policy[s])

        # For visualization, map actions to numbers
        action_to_num = {a: i for i, a in enumerate(self.actions)}

        fig, ax = plt.subplots(figsize=(9, 6))
        plt.subplots_adjust(left=0.15, right=0.85, top=0.85, bottom=0.15)

        iterations = list(range(len(self.policy_history)))
        colors = ['#766CDB', '#DA847C', '#D9CC8B', '#7CD9A5', '#877877']

        for i, s in enumerate(self.states):
            color_idx = i % len(colors)
            ax.plot(iterations, [action_to_num[action] for action in state_progress[s]],
                   label=s, marker='o', color=colors[color_idx])

        # Create a custom y-tick formatter to show action names instead of numbers
        plt.yticks(range(len(self.actions)), self.actions)

        ax.set_title("Policy Improvements Over Iterations", pad=15, fontsize=20, fontweight="semibold", color="#222222")
        ax.set_xlabel("Iteration", labelpad=10, fontsize=16, fontweight="medium", color="#333333")
        ax.set_ylabel("Action", labelpad=10, fontsize=16, fontweight="medium", color="#333333")
        ax.tick_params(axis="both", labelsize=14, colors="#555555")
        ax.set_axisbelow(True)
        ax.grid(True, linestyle="--", color="#E0E0E0", zorder=0)
        ax.legend(title="States", fontsize=12, title_fontsize=12)

        plt.show()
        print('Policy improvement visualization complete.')

    def simulate_real_time_decision(self, start_state, num_steps=10, changing_conditions=True):
        """Simulate real-time decision-making with potentially changing conditions"""
        current_state = start_state
        total_reward = 0
        path = [current_state]
        rewards = [0]  # Start with 0 reward

        # For visualization
        states_visited = []
        actions_taken = []
        rewards_received = []

        print("Starting simulation from state:", current_state)

        for step in range(num_steps):
            # Get the best action according to current policy
            action = self.policy[current_state]
            actions_taken.append(action)

            # Get reward for this state-action pair
            reward = self.R.get(current_state, {}).get(action, 0)
            total_reward += reward
            rewards_received.append(reward)

            # Determine next state based on transition probabilities
            next_state_probs = self.P[current_state][action]
            next_states = list(next_state_probs.keys())
            probs = list(next_state_probs.values())
            next_state = np.random.choice(next_states, p=probs)

            # If changing conditions, occasionally modify the environment
            if changing_conditions and step > 0 and step % 3 == 0:
                # Simulate a change in the environment by modifying a random reward
                random_state = np.random.choice(self.states)
                random_action = np.random.choice(self.actions)
                old_reward = self.R.get(random_state, {}).get(random_action, 0)
                # Change reward by a random amount between -5 and 5
                change = np.random.uniform(-5, 5)
                self.R.setdefault(random_state, {})[random_action] = old_reward + change

                print(f"Environment changed! Reward for {random_state}-{random_action} modified from {old_reward} to {self.R[random_state][random_action]}")

                # Re-run value iteration to update policy
                self.run_value_iteration(track_progress=False)
                print(f"Policy updated for changing conditions at step {step}")

            states_visited.append(current_state)
            current_state = next_state
            path.append(current_state)
            rewards.append(total_reward)

            print(f"Step {step+1}: State {states_visited[-1]} -> Action {actions_taken[-1]} -> Reward {rewards_received[-1]} -> New State {current_state}")

        # Visualize the simulation results
        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(9, 10))
        plt.subplots_adjust(left=0.15, right=0.85, top=0.9, bottom=0.1, hspace=0.3)

        # Plot cumulative reward
        ax1.plot(range(len(rewards)), rewards, marker='o', color='#766CDB')
        ax1.set_title('Cumulative Reward Over Time', pad=15, fontsize=20, fontweight='semibold', color='#222222')
        ax1.set_xlabel('Step', labelpad=10, fontsize=16, fontweight='medium', color='#333333')
        ax1.set_ylabel('Cumulative Reward', labelpad=10, fontsize=16, fontweight='medium', color='#333333')
        ax1.tick_params(axis='both', labelsize=14, colors='#555555')
        ax1.grid(True, linestyle='--', color='#E0E0E0', zorder=0)
        ax1.set_axisbelow(True)

        # Plot state-action pairs
        x = range(len(states_visited))
        ax2.scatter(x, states_visited, marker='o', color='#DA847C', label='States')

        # Create a twin axis for actions
        ax2_twin = ax2.twinx()
        ax2_twin.scatter(x, actions_taken, marker='x', color='#7CD9A5', label='Actions')

        # Set labels and title
        ax2.set_title('States and Actions Over Time', pad=15, fontsize=20, fontweight='semibold', color='#222222')
        ax2.set_xlabel('Step', labelpad=10, fontsize=16, fontweight='medium', color='#333333')
        ax2.set_ylabel('State', labelpad=10, fontsize=16, fontweight='medium', color='#333333')
        ax2_twin.set_ylabel('Action', labelpad=10, fontsize=16, fontweight='medium', color='#333333')

        # Set tick parameters
        ax2.tick_params(axis='both', labelsize=14, colors='#555555')
        ax2_twin.tick_params(axis='y', labelsize=14, colors='#555555')

        # Add grid
        ax2.grid(True, linestyle='--', color='#E0E0E0', zorder=0)
        ax2.set_axisbelow(True)

        # Create a combined legend
        lines1, labels1 = ax2.get_legend_handles_labels()
        lines2, labels2 = ax2_twin.get_legend_handles_labels()
        ax2.legend(lines1 + lines2, labels1 + labels2, loc='upper center', bbox_to_anchor=(0.5, -0.15), ncol=2)

        plt.show()
        print('Real-time decision simulation complete.')
        return path, rewards

print("Defining a simple supply chain MDP scenario...")
# Define a simple supply chain MDP scenario
states = ['S1', 'S2', 'S3', 'S4', 'S5']  # Expanded states for better visualization
actions = ['A1', 'A2', 'A3']  # Added one more action

# Transition probabilities: For each state-action pair, define a next state probability distribution
transition_probabilities = {
    'S1': {
        'A1': {'S1': 0.7, 'S2': 0.3},
        'A2': {'S2': 0.5, 'S3': 0.5},
        'A3': {'S3': 0.2, 'S4': 0.8}
    },
    'S2': {
        'A1': {'S1': 0.4, 'S2': 0.4, 'S3': 0.2},
        'A2': {'S2': 0.6, 'S3': 0.4},
        'A3': {'S4': 0.7, 'S5': 0.3}
    },
    'S3': {
        'A1': {'S1': 0.3, 'S3': 0.7},
        'A2': {'S2': 0.6, 'S3': 0.4},
        'A3': {'S4': 0.5, 'S5': 0.5}
    },
    'S4': {
        'A1': {'S3': 0.8, 'S4': 0.2},
        'A2': {'S2': 0.3, 'S5': 0.7},
        'A3': {'S1': 0.4, 'S5': 0.6}
    },
    'S5': {
        'A1': {'S4': 0.9, 'S5': 0.1},
        'A2': {'S3': 0.5, 'S5': 0.5},
        'A3': {'S1': 0.2, 'S2': 0.8}
    }
}

# Rewards: For each state-action pair, define a reward
rewards = {
    'S1': {'A1': 5, 'A2': 10, 'A3': 2},
    'S2': {'A1': -1, 'A2': 2, 'A3': 8},
    'S3': {'A1': 0, 'A2': 1, 'A3': 3},
    'S4': {'A1': 7, 'A2': -2, 'A3': 4},
    'S5': {'A1': 3, 'A2': 6, 'A3': 1}
}

print("Creating MDP instance and running value iteration...")
# Create an instance of SupplyChainMDP
mdp_instance = SupplyChainMDP(states, actions, transition_probabilities, rewards, discount=0.9, theta=1e-5)

# Run value iteration and get the results
V, policy = mdp_instance.run_value_iteration(track_progress=True)
print('Final Value Function:')
print(V)
print('Final Policy:')
print(policy)

# 1. Visualize the policy (state values and optimal actions)
print("\
1. Visualizing the state value function and optimal policy:")
mdp_instance.visualize_policy()

# 2. Visualize the optimal delivery routes
print("\
2. Visualizing the optimal delivery routes:")
mdp_instance.visualize_optimal_routes()

# 3. Visualize policy improvements over iterations
print("\
3. Visualizing policy improvements over iterations:")
mdp_instance.visualize_policy_improvements()

# 4. Simulate real-time decision-making with changing conditions
print("\
4. Simulating real-time decision-making with changing conditions:")
path, rewards = mdp_instance.simulate_real_time_decision('S1', num_steps=8, changing_conditions=True)

print("All expected outputs have been generated successfully.")

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from IPython.display import HTML, display
import os
import random

# Let's check if the dashboard file exists and examine it
dashboard_file = 'decision_making_dashboard.html'
if os.path.exists(dashboard_file):
    with open(dashboard_file, 'r') as f:
        dashboard_html = f.read()
        print(f"Dashboard file exists with {len(dashboard_html)} characters")
        # Check for image references in the HTML
        import re
        image_refs = re.findall(r'<img src="([^"]+)"', dashboard_html)
        print(f"Found {len(image_refs)} image references:")
        for img in image_refs:
            print(f"- {img}")
            # Check if these image files exist
            if os.path.exists(img):
                print(f"  File exists: {os.path.getsize(img)} bytes")
            else:
                print(f"  File does not exist")
else:
    print("Dashboard file does not exist")

# Let's also check for the mentioned image files
image_files = ['state_values.png', 'historical_performance.png']
for img in image_files:
    if os.path.exists(img):
        print(f"Image file {img} exists with size {os.path.getsize(img)} bytes")
    else:
        print(f"Image file {img} does not exist")

import streamlit as st
import os
from PIL import Image

st.set_page_config(page_title='Supply Chain Optimization Dashboard', layout='wide')

# Check if images exist and load them
state_values_img_path = 'state_values.png'
historical_performance_img_path = 'historical_performance.png'

if os.path.exists(state_values_img_path):
    state_img = Image.open(state_values_img_path)
else:
    state_img = None

if os.path.exists(historical_performance_img_path):
    hist_img = Image.open(historical_performance_img_path)
else:
    hist_img = None

# Title of the dashboard
st.title('Supply Chain Optimization Dashboard')

# Create sidebar for navigation
st.sidebar.title('Navigation')
app_mode = st.sidebar.selectbox('Choose the view', ['Dashboard', 'Simulation', 'About'])

if app_mode == 'Dashboard':
    st.header('Dashboard Overview')
    st.markdown('''
    This dashboard presents the optimal decision-making framework for our supply chain optimization.
    **Features Include:**
    - Visual representation of state values and performance metrics
    - Dynamic rendering of simulation outputs
    - Intuitive controls for interactive visualization
    ''')

    # Two columns for images
    col1, col2 = st.columns(2)
    with col1:
        st.subheader('State Values')
        if state_img:
            st.image(state_img, use_column_width=True, caption='State Values from MDP Simulation')
        else:
            st.error('State values image not found.')
    with col2:
        st.subheader('Historical Performance')
        if hist_img:
            st.image(hist_img, use_column_width=True, caption='Historical Performance Data')
        else:
            st.error('Historical performance image not found.')

    # Adding innovative interactive element: user can change the theme
    st.markdown('---')
    st.subheader('Customize Visuals')
    theme_option = st.selectbox('Select a color theme for visualizations:', ['Default', 'Dark', 'Light'])
    st.write('Theme selected: ' + theme_option)

    # Display additional interactive element: simulate random decision performance
    st.markdown('### Decision Performance Simulator')
    import numpy as np
    random_perf = np.random.normal(loc=0, scale=1, size=10)
    st.line_chart(random_perf)

elif app_mode == 'Simulation':
    st.header('Real-Time Simulation')
    st.markdown('This section simulates real-time decisions based on supply chain dynamics.')
    # Example simulation: update every 1 second
    import time
    if st.button('Start Simulation'):
        progress_bar = st.progress(0)
        simulation_text = st.empty()
        for i in range(101):
            simulation_text.text('Simulation Progress: ' + str(i) + '%')
            progress_bar.progress(i)
            time.sleep(0.05)  # simulate update delay
        st.success('Simulation Completed!')
    else:
        st.info('Press the button to start the simulation.')

elif app_mode == 'About':
    st.header('About This Dashboard')
    st.markdown('''
    Developed using **Streamlit** for an interactive experience.

    The dashboard integrates advanced optimization techniques using Markov Decision Processes (MDPs), value iteration, POMDPs, and Nash equilibrium concepts to manage supply chain operations effectively.

    **Features:**
    - Responsive layout for easy navigation
    - Interactive visualizations and simulations for real-time decision-making
    - Customization options for more personalized insights

    Created by S.JYOTHI SWAROOP
    ''')

st.markdown('---')
st.caption('S.JYOTHI SWAROOP')

print('Streamlit dashboard code executed successfully.')

!pip install pyngrok --quiet

!pip install streamlit pyngrok

# Commented out IPython magic to ensure Python compatibility.
# %%writefile jyothiswaroop.py
# 
# 
# import streamlit as st
# import os
# from PIL import Image
# import numpy as np
# import time
# 
# st.set_page_config(page_title='Supply Chain Optimization Dashboard', layout='wide')
# 
# # Check if images exist and load them
# state_values_img_path = 'state_values.png'
# historical_performance_img_path = 'historical_performance.png'
# 
# if os.path.exists(state_values_img_path):
#     state_img = Image.open(state_values_img_path)
# else:
#     state_img = None
# 
# if os.path.exists(historical_performance_img_path):
#     hist_img = Image.open(historical_performance_img_path)
# else:
#     hist_img = None
# 
# # Title of the dashboard
# st.title('Supply Chain Optimization Dashboard')
# 
# # Create sidebar for navigation
# st.sidebar.title('Navigation')
# app_mode = st.sidebar.selectbox('Choose the view', ['Dashboard', 'Simulation', 'About'])
# 
# if app_mode == 'Dashboard':
#     st.header('Dashboard Overview')
#     st.markdown('''
#     This dashboard presents the optimal decision-making framework for our supply chain optimization.
#     **Features Include:**
#     - Visual representation of state values and performance metrics
#     - Dynamic rendering of simulation outputs
#     - Intuitive controls for interactive visualization
#     ''')
# 
#     # Two columns for images
#     col1, col2 = st.columns(2)
#     with col1:
#         st.subheader('State Values')
#         if state_img:
#             st.image(state_img, use_column_width=True, caption='State Values from MDP Simulation')
#         else:
#             st.error('State values image not found.')
#     with col2:
#         st.subheader('Historical Performance')
#         if hist_img:
#             st.image(hist_img, use_column_width=True, caption='Historical Performance Data')
#         else:
#             st.error('Historical performance image not found.')
# 
#     # Adding innovative interactive element: user can change the theme
#     st.markdown('---')
#     st.subheader('Customize Visuals')
#     theme_option = st.selectbox('Select a color theme for visualizations:', ['Default', 'Dark', 'Light'])
#     st.write('Theme selected: ' + theme_option)
# 
#     # Display additional interactive element: simulate random decision performance
#     st.markdown('### Decision Performance Simulator')
#     random_perf = np.random.normal(loc=0, scale=1, size=10)
#     st.line_chart(random_perf)
# 
# elif app_mode == 'Simulation':
#     st.header('Real-Time Simulation')
#     st.markdown('This section simulates real-time decisions based on supply chain dynamics.')
#     if st.button('Start Simulation'):
#         progress_bar = st.progress(0)
#         simulation_text = st.empty()
#         for i in range(101):
#             simulation_text.text('Simulation Progress: ' + str(i) + '%')
#             progress_bar.progress(i)
#             time.sleep(0.05)  # simulate update delay
#         st.success('Simulation Completed!')
#     else:
#         st.info('Press the button to start the simulation.')
# 
# elif app_mode == 'About':
#     st.header('About This Dashboard')
#     st.markdown('''
#     Developed using **Streamlit** for an interactive experience.
# 
#     The dashboard integrates advanced optimization techniques using Markov Decision Processes (MDPs), value iteration, POMDPs, and Nash equilibrium concepts to manage supply chain operations effectively.
# 
#     **Features:**
#     - Responsive layout for easy navigation
#     - Interactive visualizations and simulations for real-time decision-making
#     - Customization options for more personalized insights
# 
#     Created by S.JYOTHI SWAROOP
#     ''')
# 
# st.markdown('---')
# st.caption('S.JYOTHI SWAROOP')
# 
# print('Streamlit dashboard code executed successfully.')
#

!pip install pyngrok

!streamlit run jyothiswaroop.py &

